<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Michael A Ghebre, PhD by MichaelGhebre</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h2>Michael A Ghebre, PhD</h2>
        <p>Welcome</p>


        <p class="view"><a href="https://github.com/MichaelGhebre">View My GitHub Profile</a></p>

      </header>
      <section>
        <h1>
<a id="about" class="anchor" href="#about" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>About</h1>

<p>I am a statistician at university of Leicester, UK. I am interested in supervised and unsupervised statistical/machine learning techniques such as classification, clustering and variable reduction using PCA or factors analysis. At the moment I am developing a variable selection method for model-based clustering, you can find the R codes in my github repository <a href="https://github.com/MichaelGhebre/ClusteringVariableSelectionUsingR">here</a>. In addition, I have a plan to implement boosting and neural network to predict class membership in high dimensional data such as gene expression (in which the number of observations are normally less than 200, but variables are more than 500,000). Please keep checking this website and I will put some examples to show how to implement these techniques using <code>R</code>, and may be in future using <code>python</code>.</p>
      </section>
      
      <section>
        <h1>
        <a id="about" class="anchor" href="#about" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Variable selection in clustering</h1>
        
 
    <p>We proposed a new method to select variables for model-based clustering (Gaussian mixture model optimized using the EM-algorithm). 
      The optimal numbers of clusters are identified using BIC (Bayesian information criterion), in which the smaller is the better. 
      Here, I will show if selecting clustering relevant variables will improve the identification of the optimal number of clusters 
      using real and simulated datasets in R. First, I will brief the theory of model-based clustering and show how the results are affected due to initialisation of the EM-algorithm</p>
        </section>
   
      <section>
        <h1>
        <a id="about" class="anchor" href="#about" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Model-based clustering</h1>
        <h2>Gaussian mixture model </h2>
        <p> A Gaussian mixture model is a model-based clustering technique in which each component probability distribution (mixture component) is corresponding to a cluster. It assumes that the observed data come from heterogeneous (two or more) populations instead of from a single (homogeneous) population,it works by modeling each of the sub-populations separately and the overall population as a mixture of these sub-populations. The optimal mixture components (clusters) and cluster memberships are estimated using maximum likelihood, which is optimized using EM algorithm.   </p>
      </section>
     
      
      <section>
        <h1>
        <a id="about" class="anchor" href="#about" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Gausian mixture in R</h1>
        <h2>R code for mixture model </h2>
        <code> 
          
#'  gmmEM 
#'
#' This is model-based clustering technique using EM algorithm which initialized using k-means or k-medoids algorithms
#' @param x a numeric  matrix or dataframe  for clusterng; c is  number of clusters;  
#' @return  Estiamted means and variance-covariance matrix in each cluster, and BIC, loglikelihood, number of iteration, clusters (class) and number of parameters
#' @export
#' @examples
#' gmmEM(iris[,1:4], c=2,initialize = c('kmeans'))
#' 


gmmEM <- function(x, c = 1:10, initialize = c("kmeans", "kmedoids", "fuzzykmeans","hierarchical"),           iter = NULL, tol = NULL, ...) {
    
    require(clusterSim)  # for initialization the kmeans clustering algorithm
    require(FactMixtAnalysis)  #  for calculating confusion matrix
    require(mvtnorm)  # for multivariate density function
    require(e1071)
    

    x <- as.matrix(x)
    
    if (is.null(x)) 
        return(NULL)
    
    # X is non-missing
    if (any(is.na(x))) {
        warning("NA's in the dataframe")
        return(NULL)
    }
    
    
    if (nrow(x) == 1) {
        warning(" The input should be more than a single observation")
        return(NULL)
    }
    
    
    if (any(!is.numeric(x))) {
        warning("The input is not a numeric dataframe or matrix")
        return(NULL)
    }
    
    
    
    n <- nrow(x)
    d <- ncol(x)
    
    
    #  hierarchical clustering 
    
    hhclust <- function(x, method ="complete", cluster = 1:20){
      
      tree <- hclust(dist(x), method = "complete")
      group <- cutree(tree,c)
      
      return(list(Call=tree, cluster=group))
    }
    
    
    
    if (c == 1 && d == 1) {
        j <- 1  # number of clusters
        
        # initialise the algorithm using k-means or k-medoids
        
        if (initialize == "kmedoids") {
            k <- pam(x, c, do.swap = F)
       } else if (initialize == "fuzzykmeans") {
          k <- cmeans(x, c, iter.max = 100,  dist = "manhattan", method="cmeans")
          
        } else if (initialize == "hierarchical") {
          
          k <- hhclust(x, cluster=c)
          
        } else {
            k <- kmeans(x, c, nstart = 1)
        }
        
        mu <- mean(x[k$cluster == 1])
        sigma <- sd(x[k$cluster == 1])
        
        p <- sum(k$cluster == 1)/length(x)
        
        loglik <- sum(log(p * (dnorm(x, mu, sigma))))
        
        p <- (2 * j - 2 + 3 * j * d + j * d^2)/2  # parameters or degree of freedom ; j = number of clusters
        bic <- -2 * loglik + p * log(n)
        
        return(list(mean = mu, sigma = sigma, loglikhood = loglik, n = n, 
            BIC = bic, df = p, clusters = j))
        
    } else if (c >= 2 & d == 1) {
        
        # initialize the EM-algorithm
        if (initialize == "kmedoids") {
            k <- pam(x, c, do.swap = F)
        } else if (initialize == "fuzzykmeans") {
          k <- cmeans(x, c, iter.max = 100,  dist = "manhattan", method="cmeans")
          
        } else if (initialize == "hierarchical") {
          k <- hhclust(x, cluster=c)
          
        } else {
            k <- kmeans(x, c, nstart = 25)
        }
        
        j <- c
        mu <- lapply(1:c, function(i) mean(x[k$cluster == i, ]))
        mu <- lapply(mu, function(x) {
            replace(x, is.na(x), .Machine$double.eps)
        })
        
        sigma <- lapply(1:c, function(i) sd(x[k$cluster == i, ]))
        sigma <- lapply(sigma, function(x) {
            replace(x, is.na(x), .Machine$double.eps)
        })
        
        
        # sigma <- lapply(sigma,function(x)
        # replace(sigma,x==0,.Machine$double.eps)) # replacing zeros sigma
        
        p <- lapply(1:c, function(i) sum(k$cluster == i)/nrow(x))
        p <- lapply(p, function(x) {
            replace(x, is.na(x), .Machine$double.eps)
        })
        
        
        Ntau <- lapply(1:c, function(i) p[[i]] * (dnorm(x, mu[[i]], sigma[[i]])))
        Ntau <- sapply(Ntau, cbind)
        SUMtau <- apply(Ntau, 1, sum)
        loglik <- sum(log(SUMtau))
        
        tol = 1e-06
        iter <- 0
        tau <- 0
        
        
        # for loop
        for (i in 1:1000) {
            
            iter <- iter + 1
            
            Ntau <- lapply(1:c, function(i) p[[i]] * (dnorm(x, mu[[i]], 
                sigma[[i]])))
            Ntau <- sapply(Ntau, cbind)
            SUMtau <- apply(Ntau, 1, sum)
            
            tau <- Ntau/SUMtau
            p <- lapply(1:c, function(i) sum(tau[, i])/nrow(x))
            p <- lapply(p, function(x) {
                replace(x, is.na(x), .Machine$double.eps)
            })
            
            mu <- lapply(1:c, function(i) sum(tau[, i] * x)/sum(tau[, i]))
            mu <- lapply(mu, function(x) {
                replace(x, is.na(x), .Machine$double.eps)
            })
            
            sigma <- lapply(1:c, function(i) sqrt(sum(tau[, i] * (x - mu[[i]])^2)/sum(tau[, 
                i])))
            sigma <- lapply(sigma, function(x) {
                replace(x, is.na(x), .Machine$double.eps)
            })
            
            loglik_0 <- loglik
            
            Ntau <- lapply(1:c, function(i) p[[i]] * (dnorm(x, mu[[i]], 
                sigma[[i]])))
            Ntau <- sapply(Ntau, cbind)
            SUMtau <- apply(Ntau, 1, sum)
            loglik <- sum(log(SUMtau))
            
            del_loglik <- loglik - loglik_0
            
            if ((abs(del_loglik) < tol) || (is.nan(abs(del_loglik)))) {
                
                break
                
            }
        }
        
        
        prob <- as.data.frame(tau)
        class <- apply(prob, 1, which.max)
        
        par <- (2 * j - 2 + 3 * j * d + j * d^2)/2  # parameters or degree of freedom; j is number of clusters
        
        bic <- -2 * loglik + par * log(n)
        
        return(list(lambda = p, mu = mu, sigma = sigma, loglikhood = loglik, 
            n = n, BIC = bic, df = par, number_of_iteration = iter, clusters = j, 
            class = class))
        
    } else if (c >= 2 & d >= 2) {
      
        # initialise the algorithm using k-means or kmedoids
        if (initialize == "kmedoids") {
          k <- pam(x, c, do.swap = F)
          
        } else if (initialize == "fuzzykmeans") {
          k <- cmeans(x, c, iter.max = 100,  dist = "manhattan", method="cmeans")
          
        } else if (initialize == "hierarchical") {
          k <- hhclust(x, cluster=c)
          
        } else {
          k <- kmeans(x, x[initial.Centers(x, c), ])
        }
        
         j <- length(unique(k$cluster))
        
    
  
        mu <- lapply(1:c, function(i) apply(x[k$cluster == i, ], 2, mean))
        sigma <- lapply(1:c, function(i) cov(x[k$cluster == i, ]))
        sigma <- lapply(sigma, function(x) sigmaFixer(x))  # fixing singularity 
        
        p <- lapply(1:c, function(i) sum(k$cluster == i)/nrow(x))
        
        tol <- 1e-06
        iter <- 0
        tau <- 0
        
        Ntau <- lapply(1:c, function(i) p[[i]] * (dmvnorm(x, mu[[i]], sigma[[i]])))
        Ntau <- sapply(Ntau, cbind)
        SUMtau <- apply(Ntau, 1, sum)
        
        loglik <- sum(log(SUMtau))
        
        # for loop
        for (i in 1:1000) {
            
            iter <- iter + 1
            
            Ntau <- lapply(1:c, function(i) p[[i]] * (dmvnorm(x, mu[[i]], 
                sigma[[i]])))
            Ntau <- sapply(Ntau, cbind)
            SUMtau <- apply(Ntau, 1, sum)
            
            tau <- Ntau/SUMtau
            p <- lapply(1:c, function(i) sum(tau[, i])/nrow(x))
            
            mu <- lapply(1:c, function(i) apply(x, 2, function(x) sum(tau[, 
                i] * x)/sum(tau[, i])))
            
            x_m <- lapply(1:c, function(i) sweep(x, 2, mu[[i]], "-"))
            
            sigma <- lapply(1:c, function(i) (t(as.matrix(x_m[[i]])) %*% 
                (as.matrix(x_m[[i]]) * tau[, i]))/sum(tau[, i]))
            sigma <- lapply(sigma, function(x) sigmaFixer(x))  # fixing singularity 
            
            loglik_0 <- loglik
            Ntau <- lapply(1:c, function(i) p[[i]] * (dmvnorm(x, mu[[i]], 
                sigma[[i]])))
            Ntau <- sapply(Ntau, cbind)
            SUMtau <- apply(Ntau, 1, sum)
            loglik <- sum(log(SUMtau))
            
            del_loglik <- loglik - loglik_0
            
            
            if ((abs(del_loglik) < tol) || (is.nan(abs(del_loglik)))) {
                
                break
                
            }
            
        }
        
        prob <- as.data.frame(tau)
        class <- apply(prob, 1, which.max)
        par <- (2 * j - 2 + 3 * j * d + j * d^2)/2  # parameters or degree of freedom
        
        bic <- -2 * loglik + par * log(n)
        
        return(list(lambda = p, mu = mu, sigma = sigma, loglikhood = loglik, 
            number_of_iteration = iter, n = n, BIC = bic, df = par, class = class))
        
    }
} 

        </code> 
      </section>
     
     
      
      
      
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">Michael A Ghebre, PhD</a></small></p>
      </footer>
    </div>
  
    <script src="javascripts/scale.fix.js"></script>
  
  </body>
  

</html>
